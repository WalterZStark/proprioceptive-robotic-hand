{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyMlbFiwTScdcs/Rlv0vvJEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WalterZStark/proprioceptive-robotic-hand/blob/main/Image_Detection_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWTH0HF_T6N4",
        "outputId": "8e518f86-6d30-4e00-b810-78ea720e0b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz to data-train/oxford-iiit-pet/images.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 791918971/791918971 [00:31<00:00, 25145901.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data-train/oxford-iiit-pet/images.tar.gz to data-train/oxford-iiit-pet\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz to data-train/oxford-iiit-pet/annotations.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19173078/19173078 [00:01<00:00, 9988756.15it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data-train/oxford-iiit-pet/annotations.tar.gz to data-train/oxford-iiit-pet\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz to data-test/oxford-iiit-pet/images.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 791918971/791918971 [00:40<00:00, 19771960.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data-test/oxford-iiit-pet/images.tar.gz to data-test/oxford-iiit-pet\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz to data-test/oxford-iiit-pet/annotations.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19173078/19173078 [00:01<00:00, 10026389.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data-test/oxford-iiit-pet/annotations.tar.gz to data-test/oxford-iiit-pet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Import Packages\n",
        "\n",
        "import torch\n",
        "import sklearn\n",
        "\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "#from sklearn.preprocessing import normalize\n",
        "\n",
        "\n",
        "# Allows torch to use gpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Get Data Transforms for ReseNet50\n",
        "train_transforms = ResNet50_Weights.DEFAULT.transforms()\n",
        "\n",
        "# # Define data transforms\n",
        "# transform = transforms.Compose([\n",
        "#       transforms.Resize((224,224)),\n",
        "#       transforms.ToTensor()])\n",
        "\n",
        "\n",
        "# Import images from Oxford Pet Dataset\n",
        "oxford_train_data = datasets.OxfordIIITPet(root = './data-train', split = \"trainval\", transform = train_transforms, download = True)\n",
        "oxford_test_data = datasets.OxfordIIITPet(root = './data-test', split = \"test\", transform = train_transforms, download = True)\n",
        "\n",
        "# Implement Data Loader\n",
        "batch = 64\n",
        "workers = 4\n",
        "data_train_loader = DataLoader(oxford_train_data,\n",
        "                          batch_size=batch,\n",
        "                          shuffle=True,\n",
        "                          num_workers=workers)\n",
        "\n",
        "data_test_loader = DataLoader(oxford_test_data,\n",
        "                          batch_size=batch,\n",
        "                          shuffle=True,\n",
        "                          num_workers=workers)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generate embeddings\n",
        "\n",
        "def generate_embeddings(oxford_train_data):\n",
        "    \"\"\"\n",
        "    Transform, resize and normalize the images and then use a pretrained model to extract\n",
        "    the embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: define a model for extraction of the embeddings (Hint: load a pretrained model,\n",
        "    #  more info here: https://pytorch.org/vision/stable/models.html)\n",
        "\n",
        "\n",
        "\n",
        "    embeddings = []\n",
        "    embedding_size = 2048 #CHANGED FROM 4096\n",
        "\n",
        "    # pick your model\n",
        "    num_images = len(oxford_train_data)\n",
        "\n",
        "    embeddings = np.zeros((num_images, embedding_size))\n",
        "\n",
        "    # TODO: Use the model to extract the embeddings. Hint: remove the last layers of the\n",
        "    # model to access the embeddings the model generates.\n",
        "\n",
        "\n",
        "\n",
        "    model = resnet50(pretrained = True)\n",
        "    model.to(device)\n",
        "    model = nn.Sequential(*(list(model.children())[:-1]))\n",
        "    i = 0\n",
        "    ran = False\n",
        "    for [X, y] in oxford_train_data:\n",
        "        '''\n",
        "        if not ran:\n",
        "\n",
        "            # Display Images\n",
        "            convert_tensor = transforms.Grayscale()\n",
        "            img_g = convert_tensor(X[0,:3,:,:])\n",
        "            print('Grayscale Image Dimensions', img_g.shape)\n",
        "\n",
        "            # display grayscale image\n",
        "            plt.imshow(img_g.permute(1, 2, 0), cmap='gray')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "            output = model(X)\n",
        "            print(output)\n",
        "            print(output.shape)\n",
        "            ran = True\n",
        "        '''\n",
        "\n",
        "        X = X.unsqueeze(0)\n",
        "        output = model(X)\n",
        "        #print(len(output.detach().numpy().flatten()))\n",
        "        #embeddings[i] = output.detach().numpy().flatten()\n",
        "\n",
        "        for j in range(0,batch):\n",
        "            if j >= len(output) or batch*i+j > num_images:\n",
        "                break\n",
        "            embeddings[batch*i+j] = output[j].detach().numpy().squeeze()\n",
        "\n",
        "\n",
        "        #print(len(embeddings[i]))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# Training sequence\n",
        "# Train Our Own Neural Network\n",
        "\n",
        "    # Loss Function\n",
        "\n",
        "\n",
        "\n",
        "# Testing sequence\n",
        "\n",
        "# Output Results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  training_embeddings_set = generate_embeddings(oxford_train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6MJ6FtPbdS7",
        "outputId": "e0998515-a2d4-4be2-ac90-ea4cd648c492"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    }
  ]
}